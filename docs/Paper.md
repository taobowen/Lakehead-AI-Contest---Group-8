## This doc lists papers related to LLM based agent

**1.	C. Shen, Y. Liu, and Q. Xiong, "The Rise and Potential of Large Language Model Based Agents: A Survey," arXiv preprint arXiv:2309.07864, 2023. [Online]. Available: https://arxiv.org/abs/2309.07864**

The paper surveys the rise of large language model (LLM)-based agents, which use advanced NLP techniques to perform tasks like question answering, summarization, and content generation. These agents are increasingly applied in areas like customer service, healthcare, and education. Despite their potential, challenges remain, such as handling ambiguity, ethical concerns, and biases. Future research aims to improve LLM robustness, reasoning, and ethical alignment. Overall, LLM agents have transformative potential, but overcoming current limitations is key to their broader adoption.

**2.	K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin, "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges," arXiv, vol. abs/2401.07339, 2024. [Online]. Available: https://arxiv.org/abs/2401.07339.**

The study demonstrates that integrating external tools through an agent framework significantly enhances code generation capabilities, particularly in complex, real-world code repositories. CodeAgent's robust performance underscores its potential in addressing repo-level coding challenges

**3.	J. J. W. Wu and F. H. Fard, "Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent," arXiv, vol. abs/2406.00215, 2024. [Online]. Available: https://arxiv.org/abs/2406.00215.**
   
The paper "Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent" examines how well large language models (LLMs) handle communication in code generation tasks. It introduces the HumanEvalComm benchmark, which includes ambiguous or incomplete problem descriptions, and defines metrics like Communication Rate to assess LLMs' ability to ask clarifying questions. The study shows that while LLMs often generate code without clarification, an agent-based approach (Okanagan) improves communication competence, leading to better code generation. The paper suggests enhancing LLM communication skills for more accurate code generation.

**4.	Y. Liu, T. Le-Cong, R. Widyasari, C. Tantithamthavorn, L. Li, X.-B. D. Le, and D. Lo, "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues," arXiv, vol. 2307.12596, 2023. [Online]. Available: https://arxiv.org/abs/2307.12596.**

The paper explores quality issues in code generated by ChatGPT, such as errors, inefficiency, and poor readability. It proposes strategies to improve this code, including post-processing, human review, prompt engineering, and training improvements. By applying these techniques, the quality of AI-generated code can be significantly enhanced, making it more reliable for real-world use.

**5.	A. Kashefi and T. Mukerji, "ChatGPT for Programming Numerical Methods," arXiv, Mar. 20, 2023. [Online]. Available: https://arxiv.org/abs/2303.12093.**

The paper explores ChatGPT's ability to generate, debug, and improve code for numerical algorithms across multiple programming languages. It highlights its strengths in code translation, debugging, and parallelization but also points out limitations like generating singular matrices, array size mismatches, and issues with long code segments. The study concludes that while ChatGPT shows promise in automating numerical method programming, further improvements are needed to address these challenges.

**6.	J. Liu, C. Xia, Y. Wang, and L. Zhang, "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation," arXiv, vol. abs/2305.01210, 2023. [Online]. Available: https://arxiv.org/abs/2305.01210.**

The paper evaluates the accuracy and reliability of ChatGPT for code generation, finding that while it can handle simple tasks, it struggles with complex ones. Common issues include logic errors, inefficiency, syntax mistakes, and poor scalability. The authors suggest improving code quality through clearer prompts, post-generation reviews, and using static analysis tools. They conclude that while ChatGPT can assist in coding, it should not be fully relied upon for complex or production-level code without thorough validation.

**7.	Y. Dong, X. Jiang, Z. Jin, and G. Li, "Self-collaboration Code Generation via ChatGPT," arXiv, vol. abs/2304.07590, 2023. [Online]. Available: https://arxiv.org/abs/2304.07590**

The paper introduces a framework where multiple ChatGPT agents collaborate to generate code. These agents take on different roles, such as analyst, coder, and tester, working together to tackle complex coding tasks. The approach is based on principles from software development, and experiments show that it improves code generation performance by 29.9% to 47.1% compared to using a single ChatGPT agent. The study demonstrates that self-collaboration among ChatGPT agents can significantly enhance code generation efficiency.

**8.	C. E. A. Coello, M. N. Alimam, and R. Kouatly, "Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models," arXiv, vol. abs/2311.02640, 2024. [Online]. Available: https://arxiv.org/abs/2311.02640.**

The paper concludes that while ChatGPT demonstrates significant potential in code generation, there are areas where it can be enhanced to match or surpass the performance of other leading LLMs.

**9.	L. Author(s), "Teaching Large Language Models to Self-Debug," arXiv, Apr. 11, 2023. [Online]. Available: https://arxiv.org/abs/2304.05128**

The paper "Teaching Large Language Models to Self-Debug" introduces a method called Self-Debugging, which enables large language models (LLMs) to identify and correct errors in their own code predictions. 

Key Contributions:Self-Debugging Method: The authors propose a technique where LLMs debug their generated code by executing it and providing natural language explanations of the code's functionality. This process allows the model to detect and understand its mistakes without external feedback.

**10.	S. Author(s), "CodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval," arXiv, Mar. 2023. [Online]. Available: https://arxiv.org/abs/2303.03004**

This paper introduces CodeEval, a comprehensive benchmark designed to evaluate code understanding, generation, translation, and retrieval across multiple programming languages. 

**11.	H. Tian, et al., "Is ChatGPT the Ultimate Programming Assistant—How Far Is It?," arXiv, Apr. 2023. [Online]. Available: https://arxiv.org/abs/2304.11938.**

This paper evaluates ChatGPT's performance as a programming assistant. It finds that ChatGPT is proficient at generating code, fixing bugs, and summarizing code. However, its success in repairing code is limited, with an average repair accuracy of 17%. The paper highlights that the model’s effectiveness heavily depends on prompt clarity, and its ability to handle complex tasks can be constrained by its attention span. While useful for common coding problems, ChatGPT still requires human oversight for more complex programming tasks.

**12.	M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Dehghani, et al., "Evaluating Large Language Models Trained on Code," arXiv, vol. abs/2107.03374, 2021. [Online]. Available: https://arxiv.org/abs/2107.03374**

This paper evaluates Codex, an LLM fine-tuned on open-source GitHub code, examining its performance on code synthesis, understanding, and generation tasks. It highlights the impact of code-specific training on the model's ability to generate functional and syntactically correct code.

**13.	A. Sobania, L. Gissl, M. Huettermann, and S. Wagner, "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT," arXiv, vol. abs/2304.10778, 2023. [Online]. Available: https://arxiv.org/abs/2304.10778.**

This empirical study compares LLM-based coding tools (GitHub Copilot, ChatGPT) in terms of their context-tracking abilities, highlighting strengths and limitations in code completion and continuation tasks, which are essential for maintaining continuity in longer code blocks.

**14.	X. Liu et al., “AgentBench: Evaluating LLMs as Agents,” Oct. 25, 2023, arXiv: arXiv:2308.03688. doi: 10.48550/arXiv.2308.03688.**

Large Language Models (LLMs) are advancing to handle real-world tasks, creating a need to assess them as agents in interactive environments. We introduce AgentBench, a dynamic benchmark with 8 environments designed to evaluate LLMs' reasoning and decision-making in multi-turn, open-ended scenarios. Testing over 27 API-based and open-source (OSS) LLMs reveals that, although top commercial LLMs show strong agent capabilities in complex settings, OSS models lag significantly. Key challenges for LLM agents include limited long-term reasoning, decision-making, and adherence to instructions. Training with code and high-quality multi-turn alignment data may enhance performance. AgentBench resources are available at https://github.com/THUDM/AgentBench.
           

**15.	A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,” Feb. 22, 2019, arXiv: arXiv:1804.07461.  [Online]. Available: http://arxiv.org/abs/1804.07461**

To maximize practical and scientific value, natural language understanding (NLU) technology needs to be general, capable of processing language without being task-specific. To this end, we present the General Language Understanding Evaluation (GLUE) benchmark, designed to evaluate models across diverse NLU tasks. GLUE is model-agnostic but promotes knowledge sharing across tasks, especially those with limited training data. We also offer a diagnostic test suite for in-depth linguistic analysis. Our evaluations of multi-task and transfer learning methods reveal that they do not yet significantly outperform individual task-specific models, highlighting the need for more robust general NLU systems.

**16.	D. Hendrycks et al., “Measuring Massive Multitask Language Understanding,” Jan. 12, 2021, arXiv: arXiv:2009.03300. Available: http://arxiv.org/abs/2009.03300**
  
This paper introduces a novel test to assess the multitask accuracy of text models across 57 diverse tasks, including elementary mathematics, US history, computer science, and law. High performance on this test requires substantial world knowledge and problem-solving skills. The study finds that while most recent models perform at near random-chance accuracy, the largest GPT-3 model exceeds random chance by about 20 percentage points. However, all models still fall short of expert-level accuracy across all tasks, with inconsistent performance and a lack of self-awareness when incorrect. Additionally, models show near-random accuracy in critical fields like morality and law. This comprehensive test provides a tool for evaluating a model’s academic and professional knowledge and identifying key limitations.

**17.	P. Liang et al., “Holistic Evaluation of Language Models,” Oct. 01, 2023, arXiv: arXiv:2211.09110. Accessed: Nov. 13, 2024. [Online]. Available: http://arxiv.org/abs/2211.09110**

This paper introduces Holistic Evaluation of Language Models (HELM) to enhance transparency in understanding the capabilities, limitations, and risks of language models (LMs). First, the study categorizes potential scenarios (use cases) and metrics (desired characteristics) for LMs, selecting a broad subset that highlights gaps (e.g., question answering for underserved dialects, trustworthiness metrics). Next, HELM employs a multi-metric approach, measuring seven metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) across 16 core scenarios 87.5% of the time to ensure comprehensive assessment beyond accuracy. It also includes seven targeted evaluations across 26 scenarios to investigate specific challenges like reasoning and disinformation. The study then evaluates 30 language models on all 42 scenarios, increasing standardized benchmarking from 17.9% to 96% coverage across models. This evaluation reveals 25 key findings, with public access to all prompts, completions, and a modular toolkit for further analysis. HELM aims to be a dynamic benchmark, evolving with new scenarios, metrics, and models for ongoing community use.

**18.	C. Qian et al., “Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents,” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds., Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 1088–1113. doi: 10.18653/v1/2024.acl-long.61.**

This paper addresses the challenge of effective user participation in language model-driven agents, which is essential given the frequent vagueness in user instructions. While these agents can devise strategies and execute tasks, they often struggle to clarify and understand precise user intentions. To address this, the study introduces Intention-in-Interaction (IN3), a benchmark focused on discerning users' implicit intentions via explicit queries. It also proposes model experts as an upstream component in agent design to improve interaction. Using IN3, the study trains Mistral-Interact, a model that proactively evaluates task ambiguity, queries user intentions, and refines them into clear goals before beginning tasks. Integrated into the XAgent framework, the enhanced system is evaluated for its understanding and execution of user instructions. Findings show the approach excels in identifying vague tasks, retrieving essential information, setting accurate execution goals, and reducing redundant tool use, ultimately enhancing efficiency.

**19.	X. Wang et al., “InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews,” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds., Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 1840–1873. doi: 10.18653/v1/2024.acl-long.102.**

Role-playing agents (RPAs), powered by large language models, are rapidly expanding, yet assessing their accuracy in reproducing target characters, or character fidelity, remains challenging. This paper presents a new approach, using psychological scales, to evaluate RPA personality fidelity rather than focusing solely on character knowledge and linguistic patterns. This study introduces InCharacter, an interview-based method for personality testing, which addresses limitations of previous self-report assessments. Experiments across various RPAs and language models, covering 32 characters and 14 psychological scales, confirm InCharacter's effectiveness. Findings reveal that state-of-the-art RPAs align closely with human-perceived personalities, reaching up to 80.7% accuracy.

**20.	S. Han, L. Chen, L.-M. Lin, Z. Xu, and K. Yu, “IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation,” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds., Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 1607–1619. doi: 10.18653/v1/2024.acl-long.88.**

Large language models excel in creating storylines and human-like character role-playing, but constraining behaviors to fit an overarching plot remains challenging. This paper introduces IBSEN, a director-actor framework that enhances plot control in drama scripts. In IBSEN, a director agent drafts plot outlines, guides actor agents in role-playing, and adjusts the storyline as human players join, ensuring alignment with plot objectives. To evaluate this approach, a unique drama plot involving multiple actor agents was developed, observing their interactions under the director's guidance. Results indicate that the framework can produce complete, diverse scripts from minimal plot outlines while preserving character consistency. The code and prompts are accessible at https://github.com/OpenDFM/ibsen.


