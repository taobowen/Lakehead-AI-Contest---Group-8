## This doc lists papers related to LLM based agent

**1.	C. Shen, Y. Liu, and Q. Xiong, "The Rise and Potential of Large Language Model Based Agents: A Survey," arXiv preprint arXiv:2309.07864, 2023. [Online]. Available: https://arxiv.org/abs/2309.07864**

The paper surveys the rise of large language model (LLM)-based agents, which use advanced NLP techniques to perform tasks like question answering, summarization, and content generation. These agents are increasingly applied in areas like customer service, healthcare, and education. Despite their potential, challenges remain, such as handling ambiguity, ethical concerns, and biases. Future research aims to improve LLM robustness, reasoning, and ethical alignment. Overall, LLM agents have transformative potential, but overcoming current limitations is key to their broader adoption.

**2.	K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin, "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges," arXiv, vol. abs/2401.07339, 2024. [Online]. Available: https://arxiv.org/abs/2401.07339.**

The study demonstrates that integrating external tools through an agent framework significantly enhances code generation capabilities, particularly in complex, real-world code repositories. CodeAgent's robust performance underscores its potential in addressing repo-level coding challenges

**3.	J. J. W. Wu and F. H. Fard, "Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent," arXiv, vol. abs/2406.00215, 2024. [Online]. Available: https://arxiv.org/abs/2406.00215.**
   
The paper "Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent" examines how well large language models (LLMs) handle communication in code generation tasks. It introduces the HumanEvalComm benchmark, which includes ambiguous or incomplete problem descriptions, and defines metrics like Communication Rate to assess LLMs' ability to ask clarifying questions. The study shows that while LLMs often generate code without clarification, an agent-based approach (Okanagan) improves communication competence, leading to better code generation. The paper suggests enhancing LLM communication skills for more accurate code generation.

**4.	Y. Liu, T. Le-Cong, R. Widyasari, C. Tantithamthavorn, L. Li, X.-B. D. Le, and D. Lo, "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues," arXiv, vol. 2307.12596, 2023. [Online]. Available: https://arxiv.org/abs/2307.12596.**

The paper explores quality issues in code generated by ChatGPT, such as errors, inefficiency, and poor readability. It proposes strategies to improve this code, including post-processing, human review, prompt engineering, and training improvements. By applying these techniques, the quality of AI-generated code can be significantly enhanced, making it more reliable for real-world use.

**5.	A. Kashefi and T. Mukerji, "ChatGPT for Programming Numerical Methods," arXiv, Mar. 20, 2023. [Online]. Available: https://arxiv.org/abs/2303.12093.**

The paper explores ChatGPT's ability to generate, debug, and improve code for numerical algorithms across multiple programming languages. It highlights its strengths in code translation, debugging, and parallelization but also points out limitations like generating singular matrices, array size mismatches, and issues with long code segments. The study concludes that while ChatGPT shows promise in automating numerical method programming, further improvements are needed to address these challenges.

**6.	J. Liu, C. Xia, Y. Wang, and L. Zhang, "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation," arXiv, vol. abs/2305.01210, 2023. [Online]. Available: https://arxiv.org/abs/2305.01210.**

The paper evaluates the accuracy and reliability of ChatGPT for code generation, finding that while it can handle simple tasks, it struggles with complex ones. Common issues include logic errors, inefficiency, syntax mistakes, and poor scalability. The authors suggest improving code quality through clearer prompts, post-generation reviews, and using static analysis tools. They conclude that while ChatGPT can assist in coding, it should not be fully relied upon for complex or production-level code without thorough validation.

**7.	Y. Dong, X. Jiang, Z. Jin, and G. Li, "Self-collaboration Code Generation via ChatGPT," arXiv, vol. abs/2304.07590, 2023. [Online]. Available: https://arxiv.org/abs/2304.07590**

The paper introduces a framework where multiple ChatGPT agents collaborate to generate code. These agents take on different roles, such as analyst, coder, and tester, working together to tackle complex coding tasks. The approach is based on principles from software development, and experiments show that it improves code generation performance by 29.9% to 47.1% compared to using a single ChatGPT agent. The study demonstrates that self-collaboration among ChatGPT agents can significantly enhance code generation efficiency.

**8.	C. E. A. Coello, M. N. Alimam, and R. Kouatly, "Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models," arXiv, vol. abs/2311.02640, 2024. [Online]. Available: https://arxiv.org/abs/2311.02640.**

The paper concludes that while ChatGPT demonstrates significant potential in code generation, there are areas where it can be enhanced to match or surpass the performance of other leading LLMs.

**9.	L. Author(s), "Teaching Large Language Models to Self-Debug," arXiv, Apr. 11, 2023. [Online]. Available: https://arxiv.org/abs/2304.05128**

The paper "Teaching Large Language Models to Self-Debug" introduces a method called Self-Debugging, which enables large language models (LLMs) to identify and correct errors in their own code predictions. 

Key Contributions:Self-Debugging Method: The authors propose a technique where LLMs debug their generated code by executing it and providing natural language explanations of the code's functionality. This process allows the model to detect and understand its mistakes without external feedback.

**10.	S. Author(s), "CodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval," arXiv, Mar. 2023. [Online]. Available: https://arxiv.org/abs/2303.03004**

This paper introduces CodeEval, a comprehensive benchmark designed to evaluate code understanding, generation, translation, and retrieval across multiple programming languages. 

**11.	H. Tian, et al., "Is ChatGPT the Ultimate Programming Assistant—How Far Is It?," arXiv, Apr. 2023. [Online]. Available: https://arxiv.org/abs/2304.11938.**

This paper evaluates ChatGPT's performance as a programming assistant. It finds that ChatGPT is proficient at generating code, fixing bugs, and summarizing code. However, its success in repairing code is limited, with an average repair accuracy of 17%. The paper highlights that the model’s effectiveness heavily depends on prompt clarity, and its ability to handle complex tasks can be constrained by its attention span. While useful for common coding problems, ChatGPT still requires human oversight for more complex programming tasks.

**12.	M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Dehghani, et al., "Evaluating Large Language Models Trained on Code," arXiv, vol. abs/2107.03374, 2021. [Online]. Available: https://arxiv.org/abs/2107.03374**

This paper evaluates Codex, an LLM fine-tuned on open-source GitHub code, examining its performance on code synthesis, understanding, and generation tasks. It highlights the impact of code-specific training on the model's ability to generate functional and syntactically correct code.

**13.	A. Sobania, L. Gissl, M. Huettermann, and S. Wagner, "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT," arXiv, vol. abs/2304.10778, 2023. [Online]. Available: https://arxiv.org/abs/2304.10778.**

This empirical study compares LLM-based coding tools (GitHub Copilot, ChatGPT) in terms of their context-tracking abilities, highlighting strengths and limitations in code completion and continuation tasks, which are essential for maintaining continuity in longer code blocks.


